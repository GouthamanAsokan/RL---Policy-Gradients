{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Policy_Gradients_Cartpole.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "1X8KlBdRUUKd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 63
        },
        "outputId": "d3a8ea7c-b4ac-4bcd-8226-ccac537756eb"
      },
      "source": [
        "#Importing the libraries\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import gym"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o4_Z_tWrUlnG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e2b586a7-2fe5-4872-a96c-a1e3dbdb8196"
      },
      "source": [
        "#creating the environment\n",
        "env = gym.make('CartPole-v0')\n",
        "env = env.unwrapped\n",
        "# Policy gradient has high variance, seed for reproducability\n",
        "env.seed(1)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tW_FxwO2UzAU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## ENVIRONMENT Hyperparameters\n",
        "state_size = 4\n",
        "action_size = env.action_space.n\n",
        "\n",
        "## TRAINING Hyperparameters\n",
        "max_episodes = 300\n",
        "learning_rate = 0.01\n",
        "gamma = 0.95 # Discount rate"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W9ji84yNVRW5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Function that takes the rewards and perform discounting\n",
        "def discount_and_normalize_rewards(episode_rewards):\n",
        "    discounted_episode_rewards = np.zeros_like(episode_rewards)\n",
        "    cumulative = 0.0\n",
        "    for i in reversed(range(len(episode_rewards))):\n",
        "        cumulative = cumulative * gamma + episode_rewards[i]\n",
        "        discounted_episode_rewards[i] = cumulative\n",
        "    \n",
        "    mean = np.mean(discounted_episode_rewards)\n",
        "    std = np.std(discounted_episode_rewards)\n",
        "    discounted_episode_rewards = (discounted_episode_rewards - mean) / (std)\n",
        "    \n",
        "    return discounted_episode_rewards"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V3Z_9N3xV2MI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "outputId": "27ff4150-add4-44ef-8a52-c31fcd72532e"
      },
      "source": [
        "#create the policy gradient neural network model\n",
        "with tf.name_scope(\"inputs\"):\n",
        "    input_ = tf.placeholder(tf.float32, [None, state_size], name=\"input_\")\n",
        "    actions = tf.placeholder(tf.int32, [None, action_size], name=\"actions\")\n",
        "    discounted_episode_rewards_ = tf.placeholder(tf.float32, [None,], name=\"discounted_episode_rewards\")\n",
        "    \n",
        "    # Add this placeholder for having this variable in tensorboard\n",
        "    mean_reward_ = tf.placeholder(tf.float32 , name=\"mean_reward\")\n",
        "\n",
        "    with tf.name_scope(\"fc1\"):\n",
        "        fc1 = tf.contrib.layers.fully_connected(inputs = input_,\n",
        "                                                num_outputs = 10,\n",
        "                                                activation_fn=tf.nn.relu,\n",
        "                                                weights_initializer=tf.contrib.layers.xavier_initializer())\n",
        "\n",
        "    with tf.name_scope(\"fc2\"):\n",
        "        fc2 = tf.contrib.layers.fully_connected(inputs = fc1,\n",
        "                                                num_outputs = action_size,\n",
        "                                                activation_fn= tf.nn.relu,\n",
        "                                                weights_initializer=tf.contrib.layers.xavier_initializer())\n",
        "    \n",
        "    with tf.name_scope(\"fc3\"):\n",
        "        fc3 = tf.contrib.layers.fully_connected(inputs = fc2,\n",
        "                                                num_outputs = action_size,\n",
        "                                                activation_fn= None,\n",
        "                                                weights_initializer=tf.contrib.layers.xavier_initializer())\n",
        "\n",
        "    with tf.name_scope(\"softmax\"):\n",
        "        action_distribution = tf.nn.softmax(fc3)\n",
        "\n",
        "    with tf.name_scope(\"loss\"):\n",
        "        # tf.nn.softmax_cross_entropy_with_logits computes the cross entropy of the result after applying the softmax function\n",
        "        # If you have single-class labels, where an object can only belong to one class, you might now consider using \n",
        "        # tf.nn.sparse_softmax_cross_entropy_with_logits so that you don't have to convert your labels to a dense one-hot array. \n",
        "        neg_log_prob = tf.nn.softmax_cross_entropy_with_logits_v2(logits = fc3, labels = actions)\n",
        "        loss = tf.reduce_mean(neg_log_prob * discounted_episode_rewards_) \n",
        "        \n",
        "    \n",
        "    with tf.name_scope(\"train\"):\n",
        "        train_opt = tf.train.AdamOptimizer(learning_rate).minimize(loss)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "WARNING:tensorflow:From /tensorflow-1.15.0/python3.6/tensorflow_core/contrib/layers/python/layers/layers.py:1866: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.__call__` method instead.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jfGlgSHyW74E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#setting up tensorboard\n",
        "# Setup TensorBoard Writer\n",
        "writer = tf.summary.FileWriter(\"/tensorboard/pg/1\")\n",
        "\n",
        "## Losses\n",
        "tf.summary.scalar(\"Loss\", loss)\n",
        "\n",
        "## Reward mean\n",
        "tf.summary.scalar(\"Reward_mean\", mean_reward_)\n",
        "\n",
        "write_op = tf.summary.merge_all()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oLcAI4aMXI4b",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "6225ceb8-cfd4-45ce-91c2-bc455f66f395"
      },
      "source": [
        "allRewards = []\n",
        "total_rewards = 0\n",
        "maximumRewardRecorded = 0\n",
        "episode = 0\n",
        "episode_states, episode_actions, episode_rewards = [],[],[]\n",
        "\n",
        "saver = tf.train.Saver()\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    \n",
        "    for episode in range(max_episodes):\n",
        "        \n",
        "        episode_rewards_sum = 0\n",
        "\n",
        "        # Launch the game\n",
        "        state = env.reset()\n",
        "        \n",
        "        #env.render()\n",
        "           \n",
        "        while True:\n",
        "            \n",
        "            # Choose action a, remember WE'RE NOT IN A DETERMINISTIC ENVIRONMENT, WE'RE OUTPUT PROBABILITIES.\n",
        "            action_probability_distribution = sess.run(action_distribution, feed_dict={input_: state.reshape([1,4])})\n",
        "            \n",
        "            action = np.random.choice(range(action_probability_distribution.shape[1]), p=action_probability_distribution.ravel())  # select action w.r.t the actions prob\n",
        "\n",
        "            # Perform a\n",
        "            new_state, reward, done, info = env.step(action)\n",
        "\n",
        "            # Store s, a, r\n",
        "            episode_states.append(state)\n",
        "                        \n",
        "            # For actions because we output only one (the index) we need 2 (1 is for the action taken)\n",
        "            # We need [0., 1.] (if we take right) not just the index\n",
        "            action_ = np.zeros(action_size)\n",
        "            action_[action] = 1\n",
        "            \n",
        "            episode_actions.append(action_)\n",
        "            \n",
        "            episode_rewards.append(reward)\n",
        "            if done:\n",
        "                # Calculate sum reward\n",
        "                episode_rewards_sum = np.sum(episode_rewards)\n",
        "                \n",
        "                allRewards.append(episode_rewards_sum)\n",
        "                \n",
        "                total_rewards = np.sum(allRewards)\n",
        "                \n",
        "                # Mean reward\n",
        "                mean_reward = np.divide(total_rewards, episode+1)\n",
        "                \n",
        "                \n",
        "                maximumRewardRecorded = np.amax(allRewards)\n",
        "                \n",
        "                print(\"==========================================\")\n",
        "                print(\"Episode: \", episode)\n",
        "                print(\"Reward: \", episode_rewards_sum)\n",
        "                print(\"Mean Reward\", mean_reward)\n",
        "                print(\"Max reward so far: \", maximumRewardRecorded)\n",
        "                \n",
        "                # Calculate discounted reward\n",
        "                discounted_episode_rewards = discount_and_normalize_rewards(episode_rewards)\n",
        "                                \n",
        "                # Feedforward, gradient and backpropagation\n",
        "                loss_, _ = sess.run([loss, train_opt], feed_dict={input_: np.vstack(np.array(episode_states)),\n",
        "                                                                 actions: np.vstack(np.array(episode_actions)),\n",
        "                                                                 discounted_episode_rewards_: discounted_episode_rewards \n",
        "                                                                })\n",
        "                \n",
        " \n",
        "                                                                 \n",
        "                # Write TF Summaries\n",
        "                summary = sess.run(write_op, feed_dict={input_: np.vstack(np.array(episode_states)),\n",
        "                                                                 actions: np.vstack(np.array(episode_actions)),\n",
        "                                                                 discounted_episode_rewards_: discounted_episode_rewards,\n",
        "                                                                    mean_reward_: mean_reward\n",
        "                                                                })\n",
        "                \n",
        "               \n",
        "                writer.add_summary(summary, episode)\n",
        "                writer.flush()\n",
        "                \n",
        "            \n",
        "                \n",
        "                # Reset the transition stores\n",
        "                episode_states, episode_actions, episode_rewards = [],[],[]\n",
        "                \n",
        "                break\n",
        "            \n",
        "            state = new_state\n",
        "        \n",
        "        # Save Model\n",
        "        if episode % 100 == 0:\n",
        "            saver.save(sess, \"./models/model.ckpt\")\n",
        "            print(\"Model saved\")\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "==========================================\n",
            "Episode:  0\n",
            "Reward:  18.0\n",
            "Mean Reward 18.0\n",
            "Max reward so far:  18.0\n",
            "Model saved\n",
            "==========================================\n",
            "Episode:  1\n",
            "Reward:  8.0\n",
            "Mean Reward 13.0\n",
            "Max reward so far:  18.0\n",
            "==========================================\n",
            "Episode:  2\n",
            "Reward:  16.0\n",
            "Mean Reward 14.0\n",
            "Max reward so far:  18.0\n",
            "==========================================\n",
            "Episode:  3\n",
            "Reward:  28.0\n",
            "Mean Reward 17.5\n",
            "Max reward so far:  28.0\n",
            "==========================================\n",
            "Episode:  4\n",
            "Reward:  9.0\n",
            "Mean Reward 15.8\n",
            "Max reward so far:  28.0\n",
            "==========================================\n",
            "Episode:  5\n",
            "Reward:  13.0\n",
            "Mean Reward 15.333333333333334\n",
            "Max reward so far:  28.0\n",
            "==========================================\n",
            "Episode:  6\n",
            "Reward:  65.0\n",
            "Mean Reward 22.428571428571427\n",
            "Max reward so far:  65.0\n",
            "==========================================\n",
            "Episode:  7\n",
            "Reward:  15.0\n",
            "Mean Reward 21.5\n",
            "Max reward so far:  65.0\n",
            "==========================================\n",
            "Episode:  8\n",
            "Reward:  26.0\n",
            "Mean Reward 22.0\n",
            "Max reward so far:  65.0\n",
            "==========================================\n",
            "Episode:  9\n",
            "Reward:  19.0\n",
            "Mean Reward 21.7\n",
            "Max reward so far:  65.0\n",
            "==========================================\n",
            "Episode:  10\n",
            "Reward:  22.0\n",
            "Mean Reward 21.727272727272727\n",
            "Max reward so far:  65.0\n",
            "==========================================\n",
            "Episode:  11\n",
            "Reward:  13.0\n",
            "Mean Reward 21.0\n",
            "Max reward so far:  65.0\n",
            "==========================================\n",
            "Episode:  12\n",
            "Reward:  13.0\n",
            "Mean Reward 20.384615384615383\n",
            "Max reward so far:  65.0\n",
            "==========================================\n",
            "Episode:  13\n",
            "Reward:  22.0\n",
            "Mean Reward 20.5\n",
            "Max reward so far:  65.0\n",
            "==========================================\n",
            "Episode:  14\n",
            "Reward:  8.0\n",
            "Mean Reward 19.666666666666668\n",
            "Max reward so far:  65.0\n",
            "==========================================\n",
            "Episode:  15\n",
            "Reward:  21.0\n",
            "Mean Reward 19.75\n",
            "Max reward so far:  65.0\n",
            "==========================================\n",
            "Episode:  16\n",
            "Reward:  31.0\n",
            "Mean Reward 20.41176470588235\n",
            "Max reward so far:  65.0\n",
            "==========================================\n",
            "Episode:  17\n",
            "Reward:  21.0\n",
            "Mean Reward 20.444444444444443\n",
            "Max reward so far:  65.0\n",
            "==========================================\n",
            "Episode:  18\n",
            "Reward:  14.0\n",
            "Mean Reward 20.105263157894736\n",
            "Max reward so far:  65.0\n",
            "==========================================\n",
            "Episode:  19\n",
            "Reward:  14.0\n",
            "Mean Reward 19.8\n",
            "Max reward so far:  65.0\n",
            "==========================================\n",
            "Episode:  20\n",
            "Reward:  15.0\n",
            "Mean Reward 19.571428571428573\n",
            "Max reward so far:  65.0\n",
            "==========================================\n",
            "Episode:  21\n",
            "Reward:  19.0\n",
            "Mean Reward 19.545454545454547\n",
            "Max reward so far:  65.0\n",
            "==========================================\n",
            "Episode:  22\n",
            "Reward:  18.0\n",
            "Mean Reward 19.47826086956522\n",
            "Max reward so far:  65.0\n",
            "==========================================\n",
            "Episode:  23\n",
            "Reward:  8.0\n",
            "Mean Reward 19.0\n",
            "Max reward so far:  65.0\n",
            "==========================================\n",
            "Episode:  24\n",
            "Reward:  18.0\n",
            "Mean Reward 18.96\n",
            "Max reward so far:  65.0\n",
            "==========================================\n",
            "Episode:  25\n",
            "Reward:  13.0\n",
            "Mean Reward 18.73076923076923\n",
            "Max reward so far:  65.0\n",
            "==========================================\n",
            "Episode:  26\n",
            "Reward:  24.0\n",
            "Mean Reward 18.925925925925927\n",
            "Max reward so far:  65.0\n",
            "==========================================\n",
            "Episode:  27\n",
            "Reward:  16.0\n",
            "Mean Reward 18.821428571428573\n",
            "Max reward so far:  65.0\n",
            "==========================================\n",
            "Episode:  28\n",
            "Reward:  16.0\n",
            "Mean Reward 18.724137931034484\n",
            "Max reward so far:  65.0\n",
            "==========================================\n",
            "Episode:  29\n",
            "Reward:  13.0\n",
            "Mean Reward 18.533333333333335\n",
            "Max reward so far:  65.0\n",
            "==========================================\n",
            "Episode:  30\n",
            "Reward:  11.0\n",
            "Mean Reward 18.29032258064516\n",
            "Max reward so far:  65.0\n",
            "==========================================\n",
            "Episode:  31\n",
            "Reward:  28.0\n",
            "Mean Reward 18.59375\n",
            "Max reward so far:  65.0\n",
            "==========================================\n",
            "Episode:  32\n",
            "Reward:  17.0\n",
            "Mean Reward 18.545454545454547\n",
            "Max reward so far:  65.0\n",
            "==========================================\n",
            "Episode:  33\n",
            "Reward:  18.0\n",
            "Mean Reward 18.529411764705884\n",
            "Max reward so far:  65.0\n",
            "==========================================\n",
            "Episode:  34\n",
            "Reward:  16.0\n",
            "Mean Reward 18.457142857142856\n",
            "Max reward so far:  65.0\n",
            "==========================================\n",
            "Episode:  35\n",
            "Reward:  15.0\n",
            "Mean Reward 18.36111111111111\n",
            "Max reward so far:  65.0\n",
            "==========================================\n",
            "Episode:  36\n",
            "Reward:  11.0\n",
            "Mean Reward 18.16216216216216\n",
            "Max reward so far:  65.0\n",
            "==========================================\n",
            "Episode:  37\n",
            "Reward:  23.0\n",
            "Mean Reward 18.289473684210527\n",
            "Max reward so far:  65.0\n",
            "==========================================\n",
            "Episode:  38\n",
            "Reward:  11.0\n",
            "Mean Reward 18.102564102564102\n",
            "Max reward so far:  65.0\n",
            "==========================================\n",
            "Episode:  39\n",
            "Reward:  10.0\n",
            "Mean Reward 17.9\n",
            "Max reward so far:  65.0\n",
            "==========================================\n",
            "Episode:  40\n",
            "Reward:  19.0\n",
            "Mean Reward 17.926829268292682\n",
            "Max reward so far:  65.0\n",
            "==========================================\n",
            "Episode:  41\n",
            "Reward:  20.0\n",
            "Mean Reward 17.976190476190474\n",
            "Max reward so far:  65.0\n",
            "==========================================\n",
            "Episode:  42\n",
            "Reward:  24.0\n",
            "Mean Reward 18.11627906976744\n",
            "Max reward so far:  65.0\n",
            "==========================================\n",
            "Episode:  43\n",
            "Reward:  17.0\n",
            "Mean Reward 18.09090909090909\n",
            "Max reward so far:  65.0\n",
            "==========================================\n",
            "Episode:  44\n",
            "Reward:  20.0\n",
            "Mean Reward 18.133333333333333\n",
            "Max reward so far:  65.0\n",
            "==========================================\n",
            "Episode:  45\n",
            "Reward:  9.0\n",
            "Mean Reward 17.934782608695652\n",
            "Max reward so far:  65.0\n",
            "==========================================\n",
            "Episode:  46\n",
            "Reward:  13.0\n",
            "Mean Reward 17.829787234042552\n",
            "Max reward so far:  65.0\n",
            "==========================================\n",
            "Episode:  47\n",
            "Reward:  25.0\n",
            "Mean Reward 17.979166666666668\n",
            "Max reward so far:  65.0\n",
            "==========================================\n",
            "Episode:  48\n",
            "Reward:  31.0\n",
            "Mean Reward 18.244897959183675\n",
            "Max reward so far:  65.0\n",
            "==========================================\n",
            "Episode:  49\n",
            "Reward:  15.0\n",
            "Mean Reward 18.18\n",
            "Max reward so far:  65.0\n",
            "==========================================\n",
            "Episode:  50\n",
            "Reward:  17.0\n",
            "Mean Reward 18.15686274509804\n",
            "Max reward so far:  65.0\n",
            "==========================================\n",
            "Episode:  51\n",
            "Reward:  28.0\n",
            "Mean Reward 18.346153846153847\n",
            "Max reward so far:  65.0\n",
            "==========================================\n",
            "Episode:  52\n",
            "Reward:  15.0\n",
            "Mean Reward 18.28301886792453\n",
            "Max reward so far:  65.0\n",
            "==========================================\n",
            "Episode:  53\n",
            "Reward:  18.0\n",
            "Mean Reward 18.27777777777778\n",
            "Max reward so far:  65.0\n",
            "==========================================\n",
            "Episode:  54\n",
            "Reward:  30.0\n",
            "Mean Reward 18.490909090909092\n",
            "Max reward so far:  65.0\n",
            "==========================================\n",
            "Episode:  55\n",
            "Reward:  16.0\n",
            "Mean Reward 18.446428571428573\n",
            "Max reward so far:  65.0\n",
            "==========================================\n",
            "Episode:  56\n",
            "Reward:  19.0\n",
            "Mean Reward 18.45614035087719\n",
            "Max reward so far:  65.0\n",
            "==========================================\n",
            "Episode:  57\n",
            "Reward:  14.0\n",
            "Mean Reward 18.379310344827587\n",
            "Max reward so far:  65.0\n",
            "==========================================\n",
            "Episode:  58\n",
            "Reward:  11.0\n",
            "Mean Reward 18.25423728813559\n",
            "Max reward so far:  65.0\n",
            "==========================================\n",
            "Episode:  59\n",
            "Reward:  10.0\n",
            "Mean Reward 18.116666666666667\n",
            "Max reward so far:  65.0\n",
            "==========================================\n",
            "Episode:  60\n",
            "Reward:  16.0\n",
            "Mean Reward 18.081967213114755\n",
            "Max reward so far:  65.0\n",
            "==========================================\n",
            "Episode:  61\n",
            "Reward:  60.0\n",
            "Mean Reward 18.758064516129032\n",
            "Max reward so far:  65.0\n",
            "==========================================\n",
            "Episode:  62\n",
            "Reward:  10.0\n",
            "Mean Reward 18.61904761904762\n",
            "Max reward so far:  65.0\n",
            "==========================================\n",
            "Episode:  63\n",
            "Reward:  13.0\n",
            "Mean Reward 18.53125\n",
            "Max reward so far:  65.0\n",
            "==========================================\n",
            "Episode:  64\n",
            "Reward:  10.0\n",
            "Mean Reward 18.4\n",
            "Max reward so far:  65.0\n",
            "==========================================\n",
            "Episode:  65\n",
            "Reward:  16.0\n",
            "Mean Reward 18.363636363636363\n",
            "Max reward so far:  65.0\n",
            "==========================================\n",
            "Episode:  66\n",
            "Reward:  20.0\n",
            "Mean Reward 18.388059701492537\n",
            "Max reward so far:  65.0\n",
            "==========================================\n",
            "Episode:  67\n",
            "Reward:  18.0\n",
            "Mean Reward 18.38235294117647\n",
            "Max reward so far:  65.0\n",
            "==========================================\n",
            "Episode:  68\n",
            "Reward:  18.0\n",
            "Mean Reward 18.3768115942029\n",
            "Max reward so far:  65.0\n",
            "==========================================\n",
            "Episode:  69\n",
            "Reward:  12.0\n",
            "Mean Reward 18.285714285714285\n",
            "Max reward so far:  65.0\n",
            "==========================================\n",
            "Episode:  70\n",
            "Reward:  23.0\n",
            "Mean Reward 18.35211267605634\n",
            "Max reward so far:  65.0\n",
            "==========================================\n",
            "Episode:  71\n",
            "Reward:  9.0\n",
            "Mean Reward 18.22222222222222\n",
            "Max reward so far:  65.0\n",
            "==========================================\n",
            "Episode:  72\n",
            "Reward:  26.0\n",
            "Mean Reward 18.328767123287673\n",
            "Max reward so far:  65.0\n",
            "==========================================\n",
            "Episode:  73\n",
            "Reward:  14.0\n",
            "Mean Reward 18.27027027027027\n",
            "Max reward so far:  65.0\n",
            "==========================================\n",
            "Episode:  74\n",
            "Reward:  14.0\n",
            "Mean Reward 18.213333333333335\n",
            "Max reward so far:  65.0\n",
            "==========================================\n",
            "Episode:  75\n",
            "Reward:  28.0\n",
            "Mean Reward 18.342105263157894\n",
            "Max reward so far:  65.0\n",
            "==========================================\n",
            "Episode:  76\n",
            "Reward:  10.0\n",
            "Mean Reward 18.233766233766232\n",
            "Max reward so far:  65.0\n",
            "==========================================\n",
            "Episode:  77\n",
            "Reward:  15.0\n",
            "Mean Reward 18.192307692307693\n",
            "Max reward so far:  65.0\n",
            "==========================================\n",
            "Episode:  78\n",
            "Reward:  16.0\n",
            "Mean Reward 18.164556962025316\n",
            "Max reward so far:  65.0\n",
            "==========================================\n",
            "Episode:  79\n",
            "Reward:  16.0\n",
            "Mean Reward 18.1375\n",
            "Max reward so far:  65.0\n",
            "==========================================\n",
            "Episode:  80\n",
            "Reward:  11.0\n",
            "Mean Reward 18.049382716049383\n",
            "Max reward so far:  65.0\n",
            "==========================================\n",
            "Episode:  81\n",
            "Reward:  21.0\n",
            "Mean Reward 18.085365853658537\n",
            "Max reward so far:  65.0\n",
            "==========================================\n",
            "Episode:  82\n",
            "Reward:  32.0\n",
            "Mean Reward 18.253012048192772\n",
            "Max reward so far:  65.0\n",
            "==========================================\n",
            "Episode:  83\n",
            "Reward:  12.0\n",
            "Mean Reward 18.178571428571427\n",
            "Max reward so far:  65.0\n",
            "==========================================\n",
            "Episode:  84\n",
            "Reward:  29.0\n",
            "Mean Reward 18.305882352941175\n",
            "Max reward so far:  65.0\n",
            "==========================================\n",
            "Episode:  85\n",
            "Reward:  9.0\n",
            "Mean Reward 18.197674418604652\n",
            "Max reward so far:  65.0\n",
            "==========================================\n",
            "Episode:  86\n",
            "Reward:  59.0\n",
            "Mean Reward 18.666666666666668\n",
            "Max reward so far:  65.0\n",
            "==========================================\n",
            "Episode:  87\n",
            "Reward:  10.0\n",
            "Mean Reward 18.568181818181817\n",
            "Max reward so far:  65.0\n",
            "==========================================\n",
            "Episode:  88\n",
            "Reward:  24.0\n",
            "Mean Reward 18.629213483146067\n",
            "Max reward so far:  65.0\n",
            "==========================================\n",
            "Episode:  89\n",
            "Reward:  18.0\n",
            "Mean Reward 18.622222222222224\n",
            "Max reward so far:  65.0\n",
            "==========================================\n",
            "Episode:  90\n",
            "Reward:  40.0\n",
            "Mean Reward 18.857142857142858\n",
            "Max reward so far:  65.0\n",
            "==========================================\n",
            "Episode:  91\n",
            "Reward:  31.0\n",
            "Mean Reward 18.98913043478261\n",
            "Max reward so far:  65.0\n",
            "==========================================\n",
            "Episode:  92\n",
            "Reward:  22.0\n",
            "Mean Reward 19.021505376344088\n",
            "Max reward so far:  65.0\n",
            "==========================================\n",
            "Episode:  93\n",
            "Reward:  10.0\n",
            "Mean Reward 18.925531914893618\n",
            "Max reward so far:  65.0\n",
            "==========================================\n",
            "Episode:  94\n",
            "Reward:  15.0\n",
            "Mean Reward 18.88421052631579\n",
            "Max reward so far:  65.0\n",
            "==========================================\n",
            "Episode:  95\n",
            "Reward:  32.0\n",
            "Mean Reward 19.020833333333332\n",
            "Max reward so far:  65.0\n",
            "==========================================\n",
            "Episode:  96\n",
            "Reward:  14.0\n",
            "Mean Reward 18.969072164948454\n",
            "Max reward so far:  65.0\n",
            "==========================================\n",
            "Episode:  97\n",
            "Reward:  39.0\n",
            "Mean Reward 19.1734693877551\n",
            "Max reward so far:  65.0\n",
            "==========================================\n",
            "Episode:  98\n",
            "Reward:  25.0\n",
            "Mean Reward 19.232323232323232\n",
            "Max reward so far:  65.0\n",
            "==========================================\n",
            "Episode:  99\n",
            "Reward:  11.0\n",
            "Mean Reward 19.15\n",
            "Max reward so far:  65.0\n",
            "==========================================\n",
            "Episode:  100\n",
            "Reward:  26.0\n",
            "Mean Reward 19.217821782178216\n",
            "Max reward so far:  65.0\n",
            "Model saved\n",
            "==========================================\n",
            "Episode:  101\n",
            "Reward:  21.0\n",
            "Mean Reward 19.235294117647058\n",
            "Max reward so far:  65.0\n",
            "==========================================\n",
            "Episode:  102\n",
            "Reward:  12.0\n",
            "Mean Reward 19.16504854368932\n",
            "Max reward so far:  65.0\n",
            "==========================================\n",
            "Episode:  103\n",
            "Reward:  27.0\n",
            "Mean Reward 19.240384615384617\n",
            "Max reward so far:  65.0\n",
            "==========================================\n",
            "Episode:  104\n",
            "Reward:  17.0\n",
            "Mean Reward 19.21904761904762\n",
            "Max reward so far:  65.0\n",
            "==========================================\n",
            "Episode:  105\n",
            "Reward:  17.0\n",
            "Mean Reward 19.19811320754717\n",
            "Max reward so far:  65.0\n",
            "==========================================\n",
            "Episode:  106\n",
            "Reward:  42.0\n",
            "Mean Reward 19.411214953271028\n",
            "Max reward so far:  65.0\n",
            "==========================================\n",
            "Episode:  107\n",
            "Reward:  31.0\n",
            "Mean Reward 19.51851851851852\n",
            "Max reward so far:  65.0\n",
            "==========================================\n",
            "Episode:  108\n",
            "Reward:  56.0\n",
            "Mean Reward 19.853211009174313\n",
            "Max reward so far:  65.0\n",
            "==========================================\n",
            "Episode:  109\n",
            "Reward:  23.0\n",
            "Mean Reward 19.881818181818183\n",
            "Max reward so far:  65.0\n",
            "==========================================\n",
            "Episode:  110\n",
            "Reward:  12.0\n",
            "Mean Reward 19.81081081081081\n",
            "Max reward so far:  65.0\n",
            "==========================================\n",
            "Episode:  111\n",
            "Reward:  24.0\n",
            "Mean Reward 19.848214285714285\n",
            "Max reward so far:  65.0\n",
            "==========================================\n",
            "Episode:  112\n",
            "Reward:  12.0\n",
            "Mean Reward 19.778761061946902\n",
            "Max reward so far:  65.0\n",
            "==========================================\n",
            "Episode:  113\n",
            "Reward:  14.0\n",
            "Mean Reward 19.728070175438596\n",
            "Max reward so far:  65.0\n",
            "==========================================\n",
            "Episode:  114\n",
            "Reward:  28.0\n",
            "Mean Reward 19.8\n",
            "Max reward so far:  65.0\n",
            "==========================================\n",
            "Episode:  115\n",
            "Reward:  22.0\n",
            "Mean Reward 19.81896551724138\n",
            "Max reward so far:  65.0\n",
            "==========================================\n",
            "Episode:  116\n",
            "Reward:  16.0\n",
            "Mean Reward 19.786324786324787\n",
            "Max reward so far:  65.0\n",
            "==========================================\n",
            "Episode:  117\n",
            "Reward:  23.0\n",
            "Mean Reward 19.8135593220339\n",
            "Max reward so far:  65.0\n",
            "==========================================\n",
            "Episode:  118\n",
            "Reward:  18.0\n",
            "Mean Reward 19.798319327731093\n",
            "Max reward so far:  65.0\n",
            "==========================================\n",
            "Episode:  119\n",
            "Reward:  25.0\n",
            "Mean Reward 19.841666666666665\n",
            "Max reward so far:  65.0\n",
            "==========================================\n",
            "Episode:  120\n",
            "Reward:  57.0\n",
            "Mean Reward 20.14876033057851\n",
            "Max reward so far:  65.0\n",
            "==========================================\n",
            "Episode:  121\n",
            "Reward:  63.0\n",
            "Mean Reward 20.5\n",
            "Max reward so far:  65.0\n",
            "==========================================\n",
            "Episode:  122\n",
            "Reward:  54.0\n",
            "Mean Reward 20.772357723577237\n",
            "Max reward so far:  65.0\n",
            "==========================================\n",
            "Episode:  123\n",
            "Reward:  29.0\n",
            "Mean Reward 20.838709677419356\n",
            "Max reward so far:  65.0\n",
            "==========================================\n",
            "Episode:  124\n",
            "Reward:  63.0\n",
            "Mean Reward 21.176\n",
            "Max reward so far:  65.0\n",
            "==========================================\n",
            "Episode:  125\n",
            "Reward:  50.0\n",
            "Mean Reward 21.404761904761905\n",
            "Max reward so far:  65.0\n",
            "==========================================\n",
            "Episode:  126\n",
            "Reward:  13.0\n",
            "Mean Reward 21.338582677165356\n",
            "Max reward so far:  65.0\n",
            "==========================================\n",
            "Episode:  127\n",
            "Reward:  35.0\n",
            "Mean Reward 21.4453125\n",
            "Max reward so far:  65.0\n",
            "==========================================\n",
            "Episode:  128\n",
            "Reward:  84.0\n",
            "Mean Reward 21.930232558139537\n",
            "Max reward so far:  84.0\n",
            "==========================================\n",
            "Episode:  129\n",
            "Reward:  29.0\n",
            "Mean Reward 21.984615384615385\n",
            "Max reward so far:  84.0\n",
            "==========================================\n",
            "Episode:  130\n",
            "Reward:  33.0\n",
            "Mean Reward 22.068702290076335\n",
            "Max reward so far:  84.0\n",
            "==========================================\n",
            "Episode:  131\n",
            "Reward:  31.0\n",
            "Mean Reward 22.136363636363637\n",
            "Max reward so far:  84.0\n",
            "==========================================\n",
            "Episode:  132\n",
            "Reward:  29.0\n",
            "Mean Reward 22.18796992481203\n",
            "Max reward so far:  84.0\n",
            "==========================================\n",
            "Episode:  133\n",
            "Reward:  47.0\n",
            "Mean Reward 22.37313432835821\n",
            "Max reward so far:  84.0\n",
            "==========================================\n",
            "Episode:  134\n",
            "Reward:  21.0\n",
            "Mean Reward 22.362962962962964\n",
            "Max reward so far:  84.0\n",
            "==========================================\n",
            "Episode:  135\n",
            "Reward:  48.0\n",
            "Mean Reward 22.551470588235293\n",
            "Max reward so far:  84.0\n",
            "==========================================\n",
            "Episode:  136\n",
            "Reward:  35.0\n",
            "Mean Reward 22.642335766423358\n",
            "Max reward so far:  84.0\n",
            "==========================================\n",
            "Episode:  137\n",
            "Reward:  29.0\n",
            "Mean Reward 22.68840579710145\n",
            "Max reward so far:  84.0\n",
            "==========================================\n",
            "Episode:  138\n",
            "Reward:  61.0\n",
            "Mean Reward 22.964028776978417\n",
            "Max reward so far:  84.0\n",
            "==========================================\n",
            "Episode:  139\n",
            "Reward:  59.0\n",
            "Mean Reward 23.22142857142857\n",
            "Max reward so far:  84.0\n",
            "==========================================\n",
            "Episode:  140\n",
            "Reward:  46.0\n",
            "Mean Reward 23.382978723404257\n",
            "Max reward so far:  84.0\n",
            "==========================================\n",
            "Episode:  141\n",
            "Reward:  38.0\n",
            "Mean Reward 23.485915492957748\n",
            "Max reward so far:  84.0\n",
            "==========================================\n",
            "Episode:  142\n",
            "Reward:  46.0\n",
            "Mean Reward 23.643356643356643\n",
            "Max reward so far:  84.0\n",
            "==========================================\n",
            "Episode:  143\n",
            "Reward:  66.0\n",
            "Mean Reward 23.9375\n",
            "Max reward so far:  84.0\n",
            "==========================================\n",
            "Episode:  144\n",
            "Reward:  39.0\n",
            "Mean Reward 24.041379310344826\n",
            "Max reward so far:  84.0\n",
            "==========================================\n",
            "Episode:  145\n",
            "Reward:  53.0\n",
            "Mean Reward 24.23972602739726\n",
            "Max reward so far:  84.0\n",
            "==========================================\n",
            "Episode:  146\n",
            "Reward:  36.0\n",
            "Mean Reward 24.31972789115646\n",
            "Max reward so far:  84.0\n",
            "==========================================\n",
            "Episode:  147\n",
            "Reward:  61.0\n",
            "Mean Reward 24.56756756756757\n",
            "Max reward so far:  84.0\n",
            "==========================================\n",
            "Episode:  148\n",
            "Reward:  57.0\n",
            "Mean Reward 24.78523489932886\n",
            "Max reward so far:  84.0\n",
            "==========================================\n",
            "Episode:  149\n",
            "Reward:  51.0\n",
            "Mean Reward 24.96\n",
            "Max reward so far:  84.0\n",
            "==========================================\n",
            "Episode:  150\n",
            "Reward:  82.0\n",
            "Mean Reward 25.337748344370862\n",
            "Max reward so far:  84.0\n",
            "==========================================\n",
            "Episode:  151\n",
            "Reward:  28.0\n",
            "Mean Reward 25.355263157894736\n",
            "Max reward so far:  84.0\n",
            "==========================================\n",
            "Episode:  152\n",
            "Reward:  50.0\n",
            "Mean Reward 25.516339869281047\n",
            "Max reward so far:  84.0\n",
            "==========================================\n",
            "Episode:  153\n",
            "Reward:  23.0\n",
            "Mean Reward 25.5\n",
            "Max reward so far:  84.0\n",
            "==========================================\n",
            "Episode:  154\n",
            "Reward:  78.0\n",
            "Mean Reward 25.838709677419356\n",
            "Max reward so far:  84.0\n",
            "==========================================\n",
            "Episode:  155\n",
            "Reward:  39.0\n",
            "Mean Reward 25.923076923076923\n",
            "Max reward so far:  84.0\n",
            "==========================================\n",
            "Episode:  156\n",
            "Reward:  13.0\n",
            "Mean Reward 25.840764331210192\n",
            "Max reward so far:  84.0\n",
            "==========================================\n",
            "Episode:  157\n",
            "Reward:  51.0\n",
            "Mean Reward 26.0\n",
            "Max reward so far:  84.0\n",
            "==========================================\n",
            "Episode:  158\n",
            "Reward:  38.0\n",
            "Mean Reward 26.07547169811321\n",
            "Max reward so far:  84.0\n",
            "==========================================\n",
            "Episode:  159\n",
            "Reward:  28.0\n",
            "Mean Reward 26.0875\n",
            "Max reward so far:  84.0\n",
            "==========================================\n",
            "Episode:  160\n",
            "Reward:  27.0\n",
            "Mean Reward 26.093167701863354\n",
            "Max reward so far:  84.0\n",
            "==========================================\n",
            "Episode:  161\n",
            "Reward:  60.0\n",
            "Mean Reward 26.30246913580247\n",
            "Max reward so far:  84.0\n",
            "==========================================\n",
            "Episode:  162\n",
            "Reward:  42.0\n",
            "Mean Reward 26.39877300613497\n",
            "Max reward so far:  84.0\n",
            "==========================================\n",
            "Episode:  163\n",
            "Reward:  52.0\n",
            "Mean Reward 26.554878048780488\n",
            "Max reward so far:  84.0\n",
            "==========================================\n",
            "Episode:  164\n",
            "Reward:  56.0\n",
            "Mean Reward 26.733333333333334\n",
            "Max reward so far:  84.0\n",
            "==========================================\n",
            "Episode:  165\n",
            "Reward:  57.0\n",
            "Mean Reward 26.91566265060241\n",
            "Max reward so far:  84.0\n",
            "==========================================\n",
            "Episode:  166\n",
            "Reward:  93.0\n",
            "Mean Reward 27.311377245508982\n",
            "Max reward so far:  93.0\n",
            "==========================================\n",
            "Episode:  167\n",
            "Reward:  44.0\n",
            "Mean Reward 27.410714285714285\n",
            "Max reward so far:  93.0\n",
            "==========================================\n",
            "Episode:  168\n",
            "Reward:  80.0\n",
            "Mean Reward 27.72189349112426\n",
            "Max reward so far:  93.0\n",
            "==========================================\n",
            "Episode:  169\n",
            "Reward:  36.0\n",
            "Mean Reward 27.770588235294117\n",
            "Max reward so far:  93.0\n",
            "==========================================\n",
            "Episode:  170\n",
            "Reward:  59.0\n",
            "Mean Reward 27.953216374269005\n",
            "Max reward so far:  93.0\n",
            "==========================================\n",
            "Episode:  171\n",
            "Reward:  32.0\n",
            "Mean Reward 27.976744186046513\n",
            "Max reward so far:  93.0\n",
            "==========================================\n",
            "Episode:  172\n",
            "Reward:  23.0\n",
            "Mean Reward 27.947976878612717\n",
            "Max reward so far:  93.0\n",
            "==========================================\n",
            "Episode:  173\n",
            "Reward:  105.0\n",
            "Mean Reward 28.39080459770115\n",
            "Max reward so far:  105.0\n",
            "==========================================\n",
            "Episode:  174\n",
            "Reward:  41.0\n",
            "Mean Reward 28.462857142857143\n",
            "Max reward so far:  105.0\n",
            "==========================================\n",
            "Episode:  175\n",
            "Reward:  59.0\n",
            "Mean Reward 28.636363636363637\n",
            "Max reward so far:  105.0\n",
            "==========================================\n",
            "Episode:  176\n",
            "Reward:  97.0\n",
            "Mean Reward 29.0225988700565\n",
            "Max reward so far:  105.0\n",
            "==========================================\n",
            "Episode:  177\n",
            "Reward:  81.0\n",
            "Mean Reward 29.314606741573034\n",
            "Max reward so far:  105.0\n",
            "==========================================\n",
            "Episode:  178\n",
            "Reward:  41.0\n",
            "Mean Reward 29.379888268156424\n",
            "Max reward so far:  105.0\n",
            "==========================================\n",
            "Episode:  179\n",
            "Reward:  38.0\n",
            "Mean Reward 29.427777777777777\n",
            "Max reward so far:  105.0\n",
            "==========================================\n",
            "Episode:  180\n",
            "Reward:  30.0\n",
            "Mean Reward 29.430939226519335\n",
            "Max reward so far:  105.0\n",
            "==========================================\n",
            "Episode:  181\n",
            "Reward:  120.0\n",
            "Mean Reward 29.928571428571427\n",
            "Max reward so far:  120.0\n",
            "==========================================\n",
            "Episode:  182\n",
            "Reward:  77.0\n",
            "Mean Reward 30.185792349726775\n",
            "Max reward so far:  120.0\n",
            "==========================================\n",
            "Episode:  183\n",
            "Reward:  42.0\n",
            "Mean Reward 30.25\n",
            "Max reward so far:  120.0\n",
            "==========================================\n",
            "Episode:  184\n",
            "Reward:  84.0\n",
            "Mean Reward 30.54054054054054\n",
            "Max reward so far:  120.0\n",
            "==========================================\n",
            "Episode:  185\n",
            "Reward:  52.0\n",
            "Mean Reward 30.655913978494624\n",
            "Max reward so far:  120.0\n",
            "==========================================\n",
            "Episode:  186\n",
            "Reward:  73.0\n",
            "Mean Reward 30.88235294117647\n",
            "Max reward so far:  120.0\n",
            "==========================================\n",
            "Episode:  187\n",
            "Reward:  94.0\n",
            "Mean Reward 31.21808510638298\n",
            "Max reward so far:  120.0\n",
            "==========================================\n",
            "Episode:  188\n",
            "Reward:  32.0\n",
            "Mean Reward 31.22222222222222\n",
            "Max reward so far:  120.0\n",
            "==========================================\n",
            "Episode:  189\n",
            "Reward:  66.0\n",
            "Mean Reward 31.405263157894737\n",
            "Max reward so far:  120.0\n",
            "==========================================\n",
            "Episode:  190\n",
            "Reward:  69.0\n",
            "Mean Reward 31.602094240837697\n",
            "Max reward so far:  120.0\n",
            "==========================================\n",
            "Episode:  191\n",
            "Reward:  47.0\n",
            "Mean Reward 31.682291666666668\n",
            "Max reward so far:  120.0\n",
            "==========================================\n",
            "Episode:  192\n",
            "Reward:  77.0\n",
            "Mean Reward 31.917098445595855\n",
            "Max reward so far:  120.0\n",
            "==========================================\n",
            "Episode:  193\n",
            "Reward:  44.0\n",
            "Mean Reward 31.97938144329897\n",
            "Max reward so far:  120.0\n",
            "==========================================\n",
            "Episode:  194\n",
            "Reward:  203.0\n",
            "Mean Reward 32.85641025641026\n",
            "Max reward so far:  203.0\n",
            "==========================================\n",
            "Episode:  195\n",
            "Reward:  98.0\n",
            "Mean Reward 33.18877551020408\n",
            "Max reward so far:  203.0\n",
            "==========================================\n",
            "Episode:  196\n",
            "Reward:  103.0\n",
            "Mean Reward 33.54314720812183\n",
            "Max reward so far:  203.0\n",
            "==========================================\n",
            "Episode:  197\n",
            "Reward:  259.0\n",
            "Mean Reward 34.68181818181818\n",
            "Max reward so far:  259.0\n",
            "==========================================\n",
            "Episode:  198\n",
            "Reward:  17.0\n",
            "Mean Reward 34.5929648241206\n",
            "Max reward so far:  259.0\n",
            "==========================================\n",
            "Episode:  199\n",
            "Reward:  218.0\n",
            "Mean Reward 35.51\n",
            "Max reward so far:  259.0\n",
            "==========================================\n",
            "Episode:  200\n",
            "Reward:  117.0\n",
            "Mean Reward 35.91542288557214\n",
            "Max reward so far:  259.0\n",
            "Model saved\n",
            "==========================================\n",
            "Episode:  201\n",
            "Reward:  366.0\n",
            "Mean Reward 37.54950495049505\n",
            "Max reward so far:  366.0\n",
            "==========================================\n",
            "Episode:  202\n",
            "Reward:  116.0\n",
            "Mean Reward 37.935960591133004\n",
            "Max reward so far:  366.0\n",
            "==========================================\n",
            "Episode:  203\n",
            "Reward:  266.0\n",
            "Mean Reward 39.05392156862745\n",
            "Max reward so far:  366.0\n",
            "==========================================\n",
            "Episode:  204\n",
            "Reward:  90.0\n",
            "Mean Reward 39.302439024390246\n",
            "Max reward so far:  366.0\n",
            "==========================================\n",
            "Episode:  205\n",
            "Reward:  125.0\n",
            "Mean Reward 39.71844660194175\n",
            "Max reward so far:  366.0\n",
            "==========================================\n",
            "Episode:  206\n",
            "Reward:  192.0\n",
            "Mean Reward 40.45410628019324\n",
            "Max reward so far:  366.0\n",
            "==========================================\n",
            "Episode:  207\n",
            "Reward:  165.0\n",
            "Mean Reward 41.05288461538461\n",
            "Max reward so far:  366.0\n",
            "==========================================\n",
            "Episode:  208\n",
            "Reward:  131.0\n",
            "Mean Reward 41.483253588516746\n",
            "Max reward so far:  366.0\n",
            "==========================================\n",
            "Episode:  209\n",
            "Reward:  262.0\n",
            "Mean Reward 42.53333333333333\n",
            "Max reward so far:  366.0\n",
            "==========================================\n",
            "Episode:  210\n",
            "Reward:  99.0\n",
            "Mean Reward 42.800947867298575\n",
            "Max reward so far:  366.0\n",
            "==========================================\n",
            "Episode:  211\n",
            "Reward:  184.0\n",
            "Mean Reward 43.466981132075475\n",
            "Max reward so far:  366.0\n",
            "==========================================\n",
            "Episode:  212\n",
            "Reward:  222.0\n",
            "Mean Reward 44.305164319248824\n",
            "Max reward so far:  366.0\n",
            "==========================================\n",
            "Episode:  213\n",
            "Reward:  124.0\n",
            "Mean Reward 44.677570093457945\n",
            "Max reward so far:  366.0\n",
            "==========================================\n",
            "Episode:  214\n",
            "Reward:  346.0\n",
            "Mean Reward 46.07906976744186\n",
            "Max reward so far:  366.0\n",
            "==========================================\n",
            "Episode:  215\n",
            "Reward:  297.0\n",
            "Mean Reward 47.24074074074074\n",
            "Max reward so far:  366.0\n",
            "==========================================\n",
            "Episode:  216\n",
            "Reward:  134.0\n",
            "Mean Reward 47.64055299539171\n",
            "Max reward so far:  366.0\n",
            "==========================================\n",
            "Episode:  217\n",
            "Reward:  223.0\n",
            "Mean Reward 48.444954128440365\n",
            "Max reward so far:  366.0\n",
            "==========================================\n",
            "Episode:  218\n",
            "Reward:  120.0\n",
            "Mean Reward 48.77168949771689\n",
            "Max reward so far:  366.0\n",
            "==========================================\n",
            "Episode:  219\n",
            "Reward:  681.0\n",
            "Mean Reward 51.64545454545455\n",
            "Max reward so far:  681.0\n",
            "==========================================\n",
            "Episode:  220\n",
            "Reward:  84.0\n",
            "Mean Reward 51.79185520361991\n",
            "Max reward so far:  681.0\n",
            "==========================================\n",
            "Episode:  221\n",
            "Reward:  502.0\n",
            "Mean Reward 53.81981981981982\n",
            "Max reward so far:  681.0\n",
            "==========================================\n",
            "Episode:  222\n",
            "Reward:  524.0\n",
            "Mean Reward 55.92825112107624\n",
            "Max reward so far:  681.0\n",
            "==========================================\n",
            "Episode:  223\n",
            "Reward:  248.0\n",
            "Mean Reward 56.785714285714285\n",
            "Max reward so far:  681.0\n",
            "==========================================\n",
            "Episode:  224\n",
            "Reward:  308.0\n",
            "Mean Reward 57.90222222222222\n",
            "Max reward so far:  681.0\n",
            "==========================================\n",
            "Episode:  225\n",
            "Reward:  247.0\n",
            "Mean Reward 58.73893805309734\n",
            "Max reward so far:  681.0\n",
            "==========================================\n",
            "Episode:  226\n",
            "Reward:  304.0\n",
            "Mean Reward 59.819383259911895\n",
            "Max reward so far:  681.0\n",
            "==========================================\n",
            "Episode:  227\n",
            "Reward:  270.0\n",
            "Mean Reward 60.74122807017544\n",
            "Max reward so far:  681.0\n",
            "==========================================\n",
            "Episode:  228\n",
            "Reward:  76.0\n",
            "Mean Reward 60.80786026200873\n",
            "Max reward so far:  681.0\n",
            "==========================================\n",
            "Episode:  229\n",
            "Reward:  225.0\n",
            "Mean Reward 61.52173913043478\n",
            "Max reward so far:  681.0\n",
            "==========================================\n",
            "Episode:  230\n",
            "Reward:  268.0\n",
            "Mean Reward 62.41558441558441\n",
            "Max reward so far:  681.0\n",
            "==========================================\n",
            "Episode:  231\n",
            "Reward:  141.0\n",
            "Mean Reward 62.75431034482759\n",
            "Max reward so far:  681.0\n",
            "==========================================\n",
            "Episode:  232\n",
            "Reward:  206.0\n",
            "Mean Reward 63.36909871244635\n",
            "Max reward so far:  681.0\n",
            "==========================================\n",
            "Episode:  233\n",
            "Reward:  186.0\n",
            "Mean Reward 63.89316239316239\n",
            "Max reward so far:  681.0\n",
            "==========================================\n",
            "Episode:  234\n",
            "Reward:  143.0\n",
            "Mean Reward 64.22978723404255\n",
            "Max reward so far:  681.0\n",
            "==========================================\n",
            "Episode:  235\n",
            "Reward:  151.0\n",
            "Mean Reward 64.59745762711864\n",
            "Max reward so far:  681.0\n",
            "==========================================\n",
            "Episode:  236\n",
            "Reward:  214.0\n",
            "Mean Reward 65.22784810126582\n",
            "Max reward so far:  681.0\n",
            "==========================================\n",
            "Episode:  237\n",
            "Reward:  180.0\n",
            "Mean Reward 65.71008403361344\n",
            "Max reward so far:  681.0\n",
            "==========================================\n",
            "Episode:  238\n",
            "Reward:  155.0\n",
            "Mean Reward 66.0836820083682\n",
            "Max reward so far:  681.0\n",
            "==========================================\n",
            "Episode:  239\n",
            "Reward:  183.0\n",
            "Mean Reward 66.57083333333334\n",
            "Max reward so far:  681.0\n",
            "==========================================\n",
            "Episode:  240\n",
            "Reward:  171.0\n",
            "Mean Reward 67.00414937759336\n",
            "Max reward so far:  681.0\n",
            "==========================================\n",
            "Episode:  241\n",
            "Reward:  151.0\n",
            "Mean Reward 67.35123966942149\n",
            "Max reward so far:  681.0\n",
            "==========================================\n",
            "Episode:  242\n",
            "Reward:  210.0\n",
            "Mean Reward 67.93827160493827\n",
            "Max reward so far:  681.0\n",
            "==========================================\n",
            "Episode:  243\n",
            "Reward:  130.0\n",
            "Mean Reward 68.19262295081967\n",
            "Max reward so far:  681.0\n",
            "==========================================\n",
            "Episode:  244\n",
            "Reward:  174.0\n",
            "Mean Reward 68.62448979591836\n",
            "Max reward so far:  681.0\n",
            "==========================================\n",
            "Episode:  245\n",
            "Reward:  183.0\n",
            "Mean Reward 69.08943089430895\n",
            "Max reward so far:  681.0\n",
            "==========================================\n",
            "Episode:  246\n",
            "Reward:  115.0\n",
            "Mean Reward 69.2753036437247\n",
            "Max reward so far:  681.0\n",
            "==========================================\n",
            "Episode:  247\n",
            "Reward:  104.0\n",
            "Mean Reward 69.41532258064517\n",
            "Max reward so far:  681.0\n",
            "==========================================\n",
            "Episode:  248\n",
            "Reward:  128.0\n",
            "Mean Reward 69.65060240963855\n",
            "Max reward so far:  681.0\n",
            "==========================================\n",
            "Episode:  249\n",
            "Reward:  112.0\n",
            "Mean Reward 69.82\n",
            "Max reward so far:  681.0\n",
            "==========================================\n",
            "Episode:  250\n",
            "Reward:  109.0\n",
            "Mean Reward 69.97609561752988\n",
            "Max reward so far:  681.0\n",
            "==========================================\n",
            "Episode:  251\n",
            "Reward:  117.0\n",
            "Mean Reward 70.16269841269842\n",
            "Max reward so far:  681.0\n",
            "==========================================\n",
            "Episode:  252\n",
            "Reward:  75.0\n",
            "Mean Reward 70.18181818181819\n",
            "Max reward so far:  681.0\n",
            "==========================================\n",
            "Episode:  253\n",
            "Reward:  123.0\n",
            "Mean Reward 70.38976377952756\n",
            "Max reward so far:  681.0\n",
            "==========================================\n",
            "Episode:  254\n",
            "Reward:  75.0\n",
            "Mean Reward 70.4078431372549\n",
            "Max reward so far:  681.0\n",
            "==========================================\n",
            "Episode:  255\n",
            "Reward:  64.0\n",
            "Mean Reward 70.3828125\n",
            "Max reward so far:  681.0\n",
            "==========================================\n",
            "Episode:  256\n",
            "Reward:  76.0\n",
            "Mean Reward 70.40466926070039\n",
            "Max reward so far:  681.0\n",
            "==========================================\n",
            "Episode:  257\n",
            "Reward:  123.0\n",
            "Mean Reward 70.60852713178295\n",
            "Max reward so far:  681.0\n",
            "==========================================\n",
            "Episode:  258\n",
            "Reward:  136.0\n",
            "Mean Reward 70.86100386100387\n",
            "Max reward so far:  681.0\n",
            "==========================================\n",
            "Episode:  259\n",
            "Reward:  73.0\n",
            "Mean Reward 70.86923076923077\n",
            "Max reward so far:  681.0\n",
            "==========================================\n",
            "Episode:  260\n",
            "Reward:  116.0\n",
            "Mean Reward 71.04214559386973\n",
            "Max reward so far:  681.0\n",
            "==========================================\n",
            "Episode:  261\n",
            "Reward:  109.0\n",
            "Mean Reward 71.18702290076335\n",
            "Max reward so far:  681.0\n",
            "==========================================\n",
            "Episode:  262\n",
            "Reward:  124.0\n",
            "Mean Reward 71.38783269961978\n",
            "Max reward so far:  681.0\n",
            "==========================================\n",
            "Episode:  263\n",
            "Reward:  146.0\n",
            "Mean Reward 71.67045454545455\n",
            "Max reward so far:  681.0\n",
            "==========================================\n",
            "Episode:  264\n",
            "Reward:  114.0\n",
            "Mean Reward 71.83018867924528\n",
            "Max reward so far:  681.0\n",
            "==========================================\n",
            "Episode:  265\n",
            "Reward:  142.0\n",
            "Mean Reward 72.09398496240601\n",
            "Max reward so far:  681.0\n",
            "==========================================\n",
            "Episode:  266\n",
            "Reward:  157.0\n",
            "Mean Reward 72.4119850187266\n",
            "Max reward so far:  681.0\n",
            "==========================================\n",
            "Episode:  267\n",
            "Reward:  170.0\n",
            "Mean Reward 72.77611940298507\n",
            "Max reward so far:  681.0\n",
            "==========================================\n",
            "Episode:  268\n",
            "Reward:  144.0\n",
            "Mean Reward 73.04089219330855\n",
            "Max reward so far:  681.0\n",
            "==========================================\n",
            "Episode:  269\n",
            "Reward:  26.0\n",
            "Mean Reward 72.86666666666666\n",
            "Max reward so far:  681.0\n",
            "==========================================\n",
            "Episode:  270\n",
            "Reward:  169.0\n",
            "Mean Reward 73.22140221402213\n",
            "Max reward so far:  681.0\n",
            "==========================================\n",
            "Episode:  271\n",
            "Reward:  167.0\n",
            "Mean Reward 73.56617647058823\n",
            "Max reward so far:  681.0\n",
            "==========================================\n",
            "Episode:  272\n",
            "Reward:  221.0\n",
            "Mean Reward 74.10622710622711\n",
            "Max reward so far:  681.0\n",
            "==========================================\n",
            "Episode:  273\n",
            "Reward:  207.0\n",
            "Mean Reward 74.5912408759124\n",
            "Max reward so far:  681.0\n",
            "==========================================\n",
            "Episode:  274\n",
            "Reward:  158.0\n",
            "Mean Reward 74.89454545454545\n",
            "Max reward so far:  681.0\n",
            "==========================================\n",
            "Episode:  275\n",
            "Reward:  145.0\n",
            "Mean Reward 75.14855072463769\n",
            "Max reward so far:  681.0\n",
            "==========================================\n",
            "Episode:  276\n",
            "Reward:  164.0\n",
            "Mean Reward 75.46931407942239\n",
            "Max reward so far:  681.0\n",
            "==========================================\n",
            "Episode:  277\n",
            "Reward:  136.0\n",
            "Mean Reward 75.68705035971223\n",
            "Max reward so far:  681.0\n",
            "==========================================\n",
            "Episode:  278\n",
            "Reward:  138.0\n",
            "Mean Reward 75.91039426523298\n",
            "Max reward so far:  681.0\n",
            "==========================================\n",
            "Episode:  279\n",
            "Reward:  145.0\n",
            "Mean Reward 76.15714285714286\n",
            "Max reward so far:  681.0\n",
            "==========================================\n",
            "Episode:  280\n",
            "Reward:  155.0\n",
            "Mean Reward 76.43772241992883\n",
            "Max reward so far:  681.0\n",
            "==========================================\n",
            "Episode:  281\n",
            "Reward:  143.0\n",
            "Mean Reward 76.67375886524823\n",
            "Max reward so far:  681.0\n",
            "==========================================\n",
            "Episode:  282\n",
            "Reward:  146.0\n",
            "Mean Reward 76.91872791519435\n",
            "Max reward so far:  681.0\n",
            "==========================================\n",
            "Episode:  283\n",
            "Reward:  135.0\n",
            "Mean Reward 77.12323943661971\n",
            "Max reward so far:  681.0\n",
            "==========================================\n",
            "Episode:  284\n",
            "Reward:  147.0\n",
            "Mean Reward 77.36842105263158\n",
            "Max reward so far:  681.0\n",
            "==========================================\n",
            "Episode:  285\n",
            "Reward:  137.0\n",
            "Mean Reward 77.57692307692308\n",
            "Max reward so far:  681.0\n",
            "==========================================\n",
            "Episode:  286\n",
            "Reward:  105.0\n",
            "Mean Reward 77.67247386759581\n",
            "Max reward so far:  681.0\n",
            "==========================================\n",
            "Episode:  287\n",
            "Reward:  45.0\n",
            "Mean Reward 77.55902777777777\n",
            "Max reward so far:  681.0\n",
            "==========================================\n",
            "Episode:  288\n",
            "Reward:  117.0\n",
            "Mean Reward 77.6955017301038\n",
            "Max reward so far:  681.0\n",
            "==========================================\n",
            "Episode:  289\n",
            "Reward:  84.0\n",
            "Mean Reward 77.71724137931035\n",
            "Max reward so far:  681.0\n",
            "==========================================\n",
            "Episode:  290\n",
            "Reward:  98.0\n",
            "Mean Reward 77.78694158075601\n",
            "Max reward so far:  681.0\n",
            "==========================================\n",
            "Episode:  291\n",
            "Reward:  91.0\n",
            "Mean Reward 77.83219178082192\n",
            "Max reward so far:  681.0\n",
            "==========================================\n",
            "Episode:  292\n",
            "Reward:  94.0\n",
            "Mean Reward 77.88737201365188\n",
            "Max reward so far:  681.0\n",
            "==========================================\n",
            "Episode:  293\n",
            "Reward:  30.0\n",
            "Mean Reward 77.72448979591837\n",
            "Max reward so far:  681.0\n",
            "==========================================\n",
            "Episode:  294\n",
            "Reward:  59.0\n",
            "Mean Reward 77.66101694915254\n",
            "Max reward so far:  681.0\n",
            "==========================================\n",
            "Episode:  295\n",
            "Reward:  23.0\n",
            "Mean Reward 77.47635135135135\n",
            "Max reward so far:  681.0\n",
            "==========================================\n",
            "Episode:  296\n",
            "Reward:  66.0\n",
            "Mean Reward 77.43771043771044\n",
            "Max reward so far:  681.0\n",
            "==========================================\n",
            "Episode:  297\n",
            "Reward:  43.0\n",
            "Mean Reward 77.32214765100672\n",
            "Max reward so far:  681.0\n",
            "==========================================\n",
            "Episode:  298\n",
            "Reward:  41.0\n",
            "Mean Reward 77.20066889632108\n",
            "Max reward so far:  681.0\n",
            "==========================================\n",
            "Episode:  299\n",
            "Reward:  43.0\n",
            "Mean Reward 77.08666666666667\n",
            "Max reward so far:  681.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s0K0VY5gYwia",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 561
        },
        "outputId": "64069fb0-e5ef-4ff9-f34c-6a9d504cfcfd"
      },
      "source": [
        "with tf.Session() as sess:\n",
        "    env.reset()\n",
        "    rewards = []\n",
        "    \n",
        "    # Load the model\n",
        "    saver.restore(sess, \"./models/model.ckpt\")\n",
        "\n",
        "    for episode in range(10):\n",
        "        state = env.reset()\n",
        "        step = 0\n",
        "        done = False\n",
        "        total_rewards = 0\n",
        "        print(\"****************************************************\")\n",
        "        print(\"EPISODE \", episode)\n",
        "\n",
        "        while True:\n",
        "            \n",
        "\n",
        "            # Choose action a, remember WE'RE NOT IN A DETERMINISTIC ENVIRONMENT, WE'RE OUTPUT PROBABILITIES.\n",
        "            action_probability_distribution = sess.run(action_distribution, feed_dict={input_: state.reshape([1,4])})\n",
        "            #print(action_probability_distribution)\n",
        "            action = np.random.choice(range(action_probability_distribution.shape[1]), p=action_probability_distribution.ravel())  # select action w.r.t the actions prob\n",
        "\n",
        "\n",
        "            new_state, reward, done, info = env.step(action)\n",
        "\n",
        "            total_rewards += reward\n",
        "\n",
        "            if done:\n",
        "                rewards.append(total_rewards)\n",
        "                print (\"Score\", total_rewards)\n",
        "                break\n",
        "            state = new_state\n",
        "    env.close()\n",
        "    print (\"Score over time: \" +  str(sum(rewards)/10))\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from ./models/model.ckpt\n",
            "****************************************************\n",
            "EPISODE  0\n",
            "Score 329.0\n",
            "****************************************************\n",
            "EPISODE  1\n",
            "Score 793.0\n",
            "****************************************************\n",
            "EPISODE  2\n",
            "Score 311.0\n",
            "****************************************************\n",
            "EPISODE  3\n",
            "Score 57.0\n",
            "****************************************************\n",
            "EPISODE  4\n",
            "Score 95.0\n",
            "****************************************************\n",
            "EPISODE  5\n",
            "Score 192.0\n",
            "****************************************************\n",
            "EPISODE  6\n",
            "Score 222.0\n",
            "****************************************************\n",
            "EPISODE  7\n",
            "Score 168.0\n",
            "****************************************************\n",
            "EPISODE  8\n",
            "Score 24.0\n",
            "****************************************************\n",
            "EPISODE  9\n",
            "Score 34.0\n",
            "Score over time: 222.5\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}